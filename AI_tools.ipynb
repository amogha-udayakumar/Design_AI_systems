{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BfWpJmTVUqkq",
    "outputId": "d58c7c1d-d3bc-434f-b063-3ca13c29b7c6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "beijing = pd.read_csv(\"Cities/Beijing_labeled.csv\").astype(\"float\")\n",
    "shenyang = pd.read_csv(\"Cities/Shenyang_labeled.csv\").astype(\"float\")\n",
    "\n",
    "guan = pd.read_csv(\"Cities/Guangzhou_labeled.csv\").astype(\"float\")\n",
    "shan = pd.read_csv(\"Cities/Shanghai_labeled.csv\").astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tWv9NKx6p8zA"
   },
   "outputs": [],
   "source": [
    "# Simple function we call to normalize the data\n",
    "def normalize(df):\n",
    "  for i in df.columns:\n",
    "    m = df[i].mean()\n",
    "    std = df[i].std()\n",
    "    df[i] = (df[i]-m)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KSE9np_xXQt6"
   },
   "outputs": [],
   "source": [
    "class knn:\n",
    "\n",
    "  def __init__(self, n):\n",
    "    self.data = None\n",
    "    self.labels = None\n",
    "    self.n = n\n",
    "\n",
    "  def fit(self, train, labels):\n",
    "    # The data and labels are saved within the model\n",
    "    self.data = train\n",
    "    self.labels = labels\n",
    "\n",
    "  def predict(self, to_predict):\n",
    "    predictions = []\n",
    "    # We loop through every data point to predict\n",
    "    for i in to_predict:\n",
    "      distances = []\n",
    "      tmp_distances = []\n",
    "      # We compute the distance between the given data point i and all of the known (training) data.\n",
    "      for x in self.data:\n",
    "        distances.append(np.linalg.norm(i-x))\n",
    "        tmp_distances.append(distances[-1])\n",
    "\n",
    "      # We sort the distances\n",
    "      distances.sort()\n",
    "      tmp_predictions = []\n",
    "\n",
    "      # We get the n closest ones\n",
    "      for ind, dist in enumerate(distances):\n",
    "        if ind == self.n:\n",
    "          break \n",
    "\n",
    "        # We get the index from the chosen data point in the unsorted list\n",
    "        # Then we append the label of that point to the list\n",
    "        # The distance of the chosen data point is set to -1 to avoid collisions\n",
    "        # If we have 3 points in our data that have exactly the same distance, setting the distance to -1 ensure we do not always get the first occuring element.\n",
    "\n",
    "        index = tmp_distances.index(dist)\n",
    "        tmp_predictions.append(self.labels[index])\n",
    "        tmp_distances[index] = -1    \n",
    "      \n",
    "      # We take the mean of the predictions and round it. So Knn, is based on majority voting. \n",
    "      m = np.mean(tmp_predictions)\n",
    "      predictions.append(round(m))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "  def score(self, test, labels):\n",
    "\n",
    "    # We decided to use the F1 score as a metric\n",
    "    pred = self.predict(test)\n",
    "\n",
    "    cm = [[0,0], [0,0]]\n",
    "    for i in range(0, len(pred)):\n",
    "      cm[int(labels[i])][int(pred[i])] += 1\n",
    "\n",
    "    print(f\"Confusion matrix: {cm}\\n\")\n",
    "\n",
    "    precision_0 = cm[0][0]/(cm[0][1]+cm[0][0])\n",
    "    recall_0 = cm[0][0]/(cm[1][0]+cm[0][0])\n",
    "    f1_0 = 2*precision_0*recall_0/(precision_0+recall_0)\n",
    "\n",
    "    print(\"-- Score 0 --\\n\")\n",
    "    print(f\"F1 score: {f1_0}\")\n",
    "    print(f\"Precision score: {precision_0}\")\n",
    "    print(f\"Recall score: {recall_0}\\n\")\n",
    "\n",
    "    precision_1 = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "    recall_1 = cm[1][1]/(cm[0][1]+cm[1][1])\n",
    "    f1_1 = 2*precision_1*recall_1/(precision_1+recall_1)\n",
    "\n",
    "    print(\"-- Score 1 --\\n\")\n",
    "    print(f\"F1 score: {f1_1}\")\n",
    "    print(f\"Precision score: {precision_1}\")\n",
    "    print(f\"Recall score: {recall_1}\\n\")\n",
    "\n",
    "    mean = (f1_0+f1_1)/2\n",
    "    print(f\"Average F1 score {mean}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DlZhakGX_XZ",
    "outputId": "41147336-335f-41d3-df1e-3f588a47c9c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Guangzhou -\n",
      "\n",
      "Confusion matrix: [[730, 536], [43, 43]]\n",
      "\n",
      "-- Score 0 --\n",
      "\n",
      "F1 score: 0.7160372731731242\n",
      "Precision score: 0.5766192733017378\n",
      "Recall score: 0.944372574385511\n",
      "\n",
      "-- Score 1 --\n",
      "\n",
      "F1 score: 0.1293233082706767\n",
      "Precision score: 0.5\n",
      "Recall score: 0.07426597582037997\n",
      "\n",
      "Average F1 score 0.42268029072190044\n",
      "\n",
      "- Shanghai -\n",
      "\n",
      "Confusion matrix: [[651, 567], [65, 68]]\n",
      "\n",
      "-- Score 0 --\n",
      "\n",
      "F1 score: 0.6732161323681489\n",
      "Precision score: 0.5344827586206896\n",
      "Recall score: 0.909217877094972\n",
      "\n",
      "-- Score 1 --\n",
      "\n",
      "F1 score: 0.17708333333333334\n",
      "Precision score: 0.5112781954887218\n",
      "Recall score: 0.10708661417322834\n",
      "\n",
      "Average F1 score 0.42514973285074115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We balance the training data\n",
    "train = beijing.append(shenyang, ignore_index=True)\n",
    "\n",
    "train_0 = train[train[\"PM_HIGH\"]==0].sample(n=796, random_state=42)\n",
    "train_1 = train[train[\"PM_HIGH\"]==1]\n",
    "\n",
    "train = train_0.append(train_1, ignore_index=True)\n",
    "\n",
    "#mean_train = train[train[\"PM_HIGH\"]==0].mean()\n",
    "#print(train[train[\"PM_HIGH\"]==0].std())\n",
    "\n",
    "#sns.pairplot(train, hue=\"PM_HIGH\")\n",
    "\n",
    "labels_train = train.loc[:, 'PM_HIGH']\n",
    "del train[\"PM_HIGH\"]\n",
    "\n",
    "normalize(train)\n",
    "\n",
    "# We used sklearn to split our data in train/validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train, labels_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# This is the code we used to fine-tune the amount of neighbours to be taken into account\n",
    "#for i in [1, 5, 10, 15, 20, 25, 30]:\n",
    "#  print(i)\n",
    "#  m = knn(i)\n",
    "#  m.fit(train_data.to_numpy(), train_labels.to_numpy())\n",
    "#  m.score(val_data.to_numpy(), val_labels.to_numpy())\n",
    "\n",
    "#mean_guan = guan[guan[\"PM_HIGH\"]==0].mean()\n",
    "#print(guan[guan[\"PM_HIGH\"]==0].std())\n",
    "\n",
    "guan_labels = guan.loc[:, 'PM_HIGH']\n",
    "del guan[\"PM_HIGH\"]\n",
    "\n",
    "normalize(guan)\n",
    "\n",
    "#mean_shan = shan[shan[\"PM_HIGH\"]==0].mean()\n",
    "#print(shan[shan[\"PM_HIGH\"]==0].std())\n",
    "\n",
    "shan_labels = shan.loc[:, 'PM_HIGH']\n",
    "del shan[\"PM_HIGH\"]\n",
    "\n",
    "normalize(shan)\n",
    "\n",
    "model = knn(10)\n",
    "model.fit(train_data.to_numpy(), train_labels.to_numpy())\n",
    "print(f\"- Guangzhou -\\n\")\n",
    "model.score(guan.to_numpy(), guan_labels.to_numpy())\n",
    "print(f\"- Shanghai -\\n\")\n",
    "model.score(shan.to_numpy(), shan_labels.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DOAS_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
