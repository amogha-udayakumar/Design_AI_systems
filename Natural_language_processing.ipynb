{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC5MMYxTLk6O"
   },
   "source": [
    "**Warmup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N5Ryr8wIuRlI"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFJ5FsE_vA1O",
    "outputId": "3c76c9bb-d1b6-42e1-b33e-614084cb03d8"
   },
   "outputs": [],
   "source": [
    "file_1 = \"dat410_europarl/europarl-v7.sv-en.lc.sv\"\n",
    "file_sv = open(file_1, \"r\")\n",
    "read_sv = file_sv.read()\n",
    "words_sv = read_sv.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vd7A0OltvQ0A",
    "outputId": "6f29918b-4377-4aaa-d953-eafca9b647a6"
   },
   "outputs": [],
   "source": [
    "file_2 = \"dat410_europarl/europarl-v7.sv-en.lc.en\"\n",
    "file_sv_en = open(file_2, \"r\")\n",
    "read_sv_en = file_sv_en.read()\n",
    "words_sv_en = read_sv_en.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LR8O57qz4jwK",
    "outputId": "54eed320-341a-49bb-a615-1fd8c2694a8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common swedish words are [('att', 9138), (',', 8875), ('och', 6950), ('i', 5599), ('som', 4958), ('för', 4699), ('det', 4524), ('av', 3979), ('är', 3802), ('en', 3632)]\n",
      "The 10 most common english words are [('the', 18174), (',', 13513), ('of', 9273), ('to', 8770), ('and', 6888), ('in', 5603), ('is', 4358), ('that', 4197), ('a', 4176), ('for', 2854)]\n"
     ]
    }
   ],
   "source": [
    "c_sv = collections.Counter(words_sv)\n",
    "c_sv_en = collections.Counter(words_sv_en)\n",
    "print(\"The 10 most common swedish words are\",c_sv.most_common(10))\n",
    "print(\"The 10 most common english words are\",c_sv_en.most_common(10))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwsaBLYV_Xtz",
    "outputId": "04414b24-c47f-4d0e-f069-d9d8650d50e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that the ramdomnly chosen word is 'speaker' is 3.683553302858069e-05\n"
     ]
    }
   ],
   "source": [
    "freq_speaker = c_sv_en['speaker']\n",
    "count_sv_en= len(words_sv_en)\n",
    "prob_speaker = freq_speaker/count_sv_en\n",
    "print(\"The probability that the ramdomnly chosen word is 'speaker' is\", prob_speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3B2B0jsMBlvl",
    "outputId": "9fd7c490-6be7-467b-ca5a-15a7c5feb255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that the ramdomnly chosen word is 'zebra' is 0.0\n"
     ]
    }
   ],
   "source": [
    "freq_zebra = c_sv_en['zebra']\n",
    "count_sv_en= len(words_sv_en)\n",
    "prob_zebra = freq_zebra/count_sv_en\n",
    "print(\"The probability that the ramdomnly chosen word is 'zebra' is\", prob_zebra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ58nBu2Gnu4"
   },
   "source": [
    "Language Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tS_dt5T_i4kK",
    "outputId": "2dd8dfd7-e87a-41f1-cb52-6800d58702be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('att', 9138), (',', 8875), ('och', 6950), ('i', 5599), ('som', 4958), ('för', 4699), ('det', 4524), ('av', 3979), ('är', 3802), ('en', 3632)]\n",
      "[('the', 18174), (',', 13513), ('of', 9273), ('to', 8770), ('and', 6888), ('in', 5603), ('is', 4358), ('that', 4197), ('a', 4176), ('for', 2854)]\n"
     ]
    }
   ],
   "source": [
    "word_lists = []\n",
    "data_sets = []\n",
    "\n",
    "for i in [words_sv, words_sv_en]:\n",
    "    data_sets.append(i)\n",
    "    c = Counter(i)\n",
    "    print(c.most_common(10))\n",
    "    word_lists.append(c)\n",
    "    \n",
    "all_words = word_lists[0]\n",
    "\n",
    "# This piece of code is used when reading all of the different texts as it will merge all the counts of all the words in English\n",
    "for i in word_lists[1:]:\n",
    "    for key, item in i.items():\n",
    "        all_words[key] += item\n",
    "\n",
    "sum_words = sum(all_words.values())\n",
    "\n",
    "def mle(sentence, datasets):\n",
    "    sentence = sentence.split(\" \")\n",
    "    probability = 1\n",
    "    \n",
    "    for ind, word in enumerate(sentence[1:]):\n",
    "        \n",
    "        ind += 1\n",
    "        count_prev_word = all_words[sentence[ind-1]]\n",
    "        count_word_serie = 0\n",
    "\n",
    "        for dset in datasets:\n",
    "            for ind_dset, word_in_dset in enumerate(dset):\n",
    "                if word_in_dset == word and sentence[ind-1] == dset[ind_dset-1]:\n",
    "                    count_word_serie += 1\n",
    "        \n",
    "        if count_prev_word != 0:\n",
    "          if count_word_serie/count_prev_word != 0:\n",
    "            print(f\"Probability suite {sentence[ind-1:ind+1]} {count_word_serie/count_prev_word}\")  \n",
    "          probability *= count_word_serie/count_prev_word\n",
    "        \n",
    "        \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxmyDD8rwJ7y"
   },
   "source": [
    "**Translation Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BV-uMQ8EwXyk",
    "outputId": "8a296a85-c555-4a99-ee3e-2afceb44062b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('europeiska', 'european'): 0.9798412135365058, ('önska', 'european'): 0.8036888021525307, ('trevlig', 'european'): 0.13347034976322272, ('semester', 'european'): 0.03917648611404948, ('den', 'european'): 0.010834776603882023, ('en', 'european'): 0.005412086338315352, ('framställning', 'european'): 0.0015542243436955634, ('fransk', 'european'): 0.0015445543634413153, ('uppmaning', 'european'): 0.0014756355179120608, ('besökte', 'european'): 0.0014406438501250753}\n"
     ]
    }
   ],
   "source": [
    "file_sv = open(file_1, \"r\")\n",
    "lines_sv = file_sv.readlines()\n",
    "file_sv_en = open(file_2, \"r\")\n",
    "lines_sv_en = file_sv_en.readlines()\n",
    "pairs =list(zip(lines_sv, lines_sv_en))  #pairs of swedish-english lines\n",
    "\n",
    "\n",
    "#Initialization\n",
    "t = defaultdict(float)\n",
    "for pair in pairs:\n",
    "    sv_sent = pair[0]  #only swedish sentence\n",
    "    en_sent = pair[1]  #only english sentence\n",
    "    sv_words = sv_sent.split(' ')  #swedish words\n",
    "    en_words = en_sent.split(' ')  #english words\n",
    "    en_words.append(None)\n",
    "    for sv_word in sv_words:\n",
    "        for en_word in en_words:\n",
    "            t[(sv_word,en_word)] = 1 #initial probability\n",
    "            \n",
    "\n",
    "for i in range(10):\n",
    "    count_sv_en = defaultdict(float) #c(e,s)\n",
    "    count_en =defaultdict(float) #c(e)\n",
    "    for pair in pairs:\n",
    "        sv_sent = pair[0]\n",
    "        en_sent = pair[1]\n",
    "        sv_words = sv_sent.split(' ')\n",
    "        en_words = en_sent.split(' ')\n",
    "        en_words.append(None)\n",
    "        for sv_word in sv_words:\n",
    "            deno = defaultdict(float)\n",
    "            deno[sv_word]= 0.0\n",
    "            for en_word in en_words:\n",
    "                deno[sv_word]+=t[(sv_word,en_word)] # sum(t(s|e))--sum of probability of swedish word with each the english word in the sentence\n",
    "            for en_word in en_words:\n",
    "                nume= t[(sv_word,en_word)] # t(s|e) - probability of swedish word with that english word\n",
    "                align=nume/deno[sv_word] # alignment probability\n",
    "                count_sv_en[(en_word,sv_word)]+=align #updating pseudocount\n",
    "                count_en[(en_word)]+= align #updating pseudocount\n",
    "        for sv_word in sv_words:\n",
    "            for en_word in en_words:\n",
    "                t[(sv_word,en_word)] = count_sv_en[(en_word,sv_word)]/count_en[(en_word)]\n",
    "    \n",
    "tp={}\n",
    "eng =[]\n",
    "for key in t.keys():\n",
    "    if key[1] == 'european':\n",
    "        eng.append(key)        \n",
    "bestprob = 0\n",
    "bestmatch =None\n",
    "for e in eng:\n",
    "    tp[e] = t[e]\n",
    "\n",
    "marklist = sorted(tp.items(), key=lambda x:x[1])\n",
    "marklist.reverse() \n",
    "sortdict = dict(marklist[:10])\n",
    "print(sortdict) #print the 10 most probable words that are aligned with 'european' in descending order                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpDEGPsnStwh"
   },
   "source": [
    "**Decoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0c2noo9S14V",
    "outputId": "5e172039-be2e-483a-c188-f71e07acef71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['you']\n",
      "Word: familiar\n",
      "Probability suite ['you', 'aware'] 0.0013245033112582781\n",
      "['you', 'aware']\n",
      "Word: modest\n",
      "['you', 'aware', 'modest']\n",
      "Word: from\n",
      "['you', 'aware', 'modest', 'from']\n",
      "Probability suite ['from', 'media'] 0.0014492753623188406\n",
      "Probability suite ['from', 'there'] 0.0014492753623188406\n",
      "['you', 'aware', 'modest', 'from', 'media']\n",
      "Probability suite ['media', 'that'] 0.047619047619047616\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that']\n",
      "Probability suite ['that', 'it'] 0.05456278294019538\n",
      "Probability suite ['that', 'there'] 0.02811532046700024\n",
      "Probability suite ['that', 'fact'] 0.0004765308553728854\n",
      "Probability suite ['that', 'is'] 0.04384083869430545\n",
      "Probability suite ['that', 'time'] 0.0011913271384322134\n",
      "Probability suite ['that', 'are'] 0.007862759113652609\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that', 'it']\n",
      "Word: television\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that', 'it', 'television']\n",
      "Word: a\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that', 'it', 'television', 'a']\n",
      "Probability suite ['a', 'series'] 0.0021423470602237563\n",
      "Probability suite ['a', 'range'] 0.0014282313734825041\n",
      "Probability suite ['a', 'press'] 0.00023803856224708403\n",
      "Probability suite ['a', 'false'] 0.00023803856224708403\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that', 'it', 'television', 'a', 'series']\n",
      "Word: been\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that', 'it', 'television', 'a', 'series', 'been']\n",
      "Probability suite ['been', 'and'] 0.002849002849002849\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that', 'it', 'television', 'a', 'series', 'been', 'and']\n",
      "Word: murders\n",
      "Probability suite ['and', 'be'] 0.000725689404934688\n",
      "Probability suite ['and', 'television'] 0.00014513788098693758\n",
      "Probability suite ['and', 'assassinations'] 0.00014513788098693758\n",
      "Probability suite ['and', 'killings'] 0.00014513788098693758\n",
      "Probability suite ['and', 'from'] 0.0011611030478955006\n",
      "Probability suite ['and', '300'] 0.00014513788098693758\n",
      "Probability suite ['and', 'will'] 0.0046444121915820025\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that', 'it', 'television', 'a', 'series', 'been', 'and', 'will']\n",
      "Probability suite ['will', 'in'] 0.0027100271002710027\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that', 'it', 'television', 'a', 'series', 'been', 'and', 'will', 'in']\n",
      "Word: lanka\n",
      "Probability suite ['in', 'sri'] 0.0005218298834579927\n",
      "Probability suite ['in', 'parliament'] 0.0060880153070099145\n",
      "Probability suite ['in', 'mr'] 0.00017394329448599757\n",
      "Probability suite ['in', 'there'] 0.00017394329448599757\n",
      "Probability suite ['in', 'one'] 0.0020873195338319706\n",
      "Probability suite ['in', 'and'] 0.00034788658897199514\n",
      "Probability suite ['in', 'just'] 0.00017394329448599757\n",
      "Probability suite ['in', 'months'] 0.00017394329448599757\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that', 'it', 'television', 'a', 'series', 'been', 'and', 'will', 'in', 'parliament']\n",
      "Word: lanka\n",
      "Probability suite ['parliament', 'had'] 0.0029585798816568047\n",
      "Probability suite ['parliament', 'be'] 0.0014792899408284023\n",
      "Probability suite ['parliament', 'was'] 0.010355029585798817\n",
      "Probability suite ['parliament', 'one'] 0.0014792899408284023\n",
      "Probability suite ['parliament', 'and'] 0.08431952662721894\n",
      "Probability suite ['parliament', 'just'] 0.0029585798816568047\n",
      "Probability suite ['parliament', 'from'] 0.0014792899408284023\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that', 'it', 'television', 'a', 'series', 'been', 'and', 'will', 'in', 'parliament', 'and']\n",
      "Word: closed\n",
      "Probability suite ['and', 'perfectly'] 0.00014513788098693758\n",
      "['you', 'aware', 'modest', 'from', 'media', 'that', 'it', 'television', 'a', 'series', 'been', 'and', 'will', 'in', 'parliament', 'and', 'perfectly']\n"
     ]
    }
   ],
   "source": [
    "# Based on the translation modelling we get the n most probable translations\n",
    "def get_most_probable_translation(word, t, number):  \n",
    "  tp={}\n",
    "  eng =[]\n",
    "  for key in t.keys():\n",
    "      if key[0] == word:\n",
    "          eng.append(key)        \n",
    "  bestprob = 0\n",
    "  bestmatch =None\n",
    "  for e in eng:\n",
    "      tp[e] = t[e]\n",
    "\n",
    "  marklist = sorted(tp.items(), key=lambda x:x[1])\n",
    "  marklist.reverse()\n",
    "  #print(marklist)\n",
    "  sortdict = dict(marklist[:number])\n",
    "  return sortdict\n",
    "\n",
    "# These are the test sentences\n",
    "# sentence = \"ni har begärt en debatt i ämnet under sammanträdesperiodens kommande dagar .\"\n",
    "# sentence = \"jag ber er resa er för en tyst minut .\"\n",
    "sentence = \"ni känner till från media att det skett en rad bombexplosioner och mord i sri lanka .\"\n",
    "\n",
    "sentence = sentence.split(\" \")\n",
    "translation = []\n",
    "\n",
    "for ind, word in enumerate(sentence):\n",
    "  print(translation)\n",
    "  if ind == 0: # if we have the first word, we add its most probable translation to the string\n",
    "    possible_words = get_most_probable_translation(word, t, 1)\n",
    "    for k in possible_words.keys():\n",
    "     translation.append(k[1])\n",
    "  else:\n",
    "    possible_words = get_most_probable_translation(word, t, 20) # We get the n most probable translations\n",
    "    word = \"\"\n",
    "    highest_score = 0\n",
    "    for k in possible_words.keys(): # For each translation we get its probability, we keep the most probable word\n",
    "      if k[1] != None:\n",
    "        sentence_to_test = translation[-1]+\" \"+k[1]\n",
    "        \n",
    "        prob = mle(sentence_to_test, data_sets)\n",
    "        if prob > highest_score:\n",
    "          word = k[1]\n",
    "          highest_score = prob\n",
    "        if word == \"\": # If the word is empty we assign the first translation i.e. the \"best\" translation to it such that if none of the translations are valid, we simply translate it.\n",
    "          word = list(possible_words.keys())[0][1]\n",
    "          print(f\"Word: {word}\")\n",
    "\n",
    "    #print(word)\n",
    "    translation.append(word)\n",
    "\n",
    "print(translation)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Assignment_4_DOAI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
